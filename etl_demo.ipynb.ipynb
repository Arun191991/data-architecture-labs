{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66524aa1-5ab6-4739-b985-cb7988be6420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", True).load(\"dbfs:/FileStore/datasets/authors_publications.csv\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945c2458-a00f-40f6-bd0d-207973665e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Example: Filter rows where 'publication_year' >= 2020 and aggregate count by 'author'\n",
    "filtered_df = df.filter(F.col(\"Year\") >= 2020)\n",
    "agg_df = filtered_df.groupBy(\"Authors\").agg(F.count(\"*\").alias(\"publication_count\"))\n",
    "display(agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41041d72-1575-4722-bf17-ea4dfa3a463f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ------------------------------\n",
    "# Data Quality Validation\n",
    "# ------------------------------\n",
    "\n",
    "# Define validation rules:\n",
    "# 1. 'Authors' must not be null or empty\n",
    "# 2. 'Year' must be a valid integer and >= 2020\n",
    "validation_expr = (\n",
    "    (F.col(\"Authors\").isNull()) |\n",
    "    (F.trim(F.col(\"Authors\")) == \"\") |\n",
    "    (F.col(\"Year\").isNull()) |\n",
    "    ((F.col(\"Year\") < 2020) & F.col(\"Year\").isNotNull())\n",
    ")\n",
    "\n",
    "# Capture rows that violate any rule\n",
    "invalid_df = df.filter(validation_expr)\n",
    "\n",
    "# If any invalid rows exist, raise an error with details\n",
    "invalid_count = invalid_df.count()\n",
    "if invalid_count == 0:\n",
    "    raise ValueError(\n",
    "        f\"Data quality validation failed: {invalid_count} row(s) do not meet the criteria.\"\n",
    "    )\n",
    "\n",
    "# ------------------------------\n",
    "# Continue with clean data processing\n",
    "# ------------------------------\n",
    "\n",
    "# Filter rows that passed validation (i.e., Year >= 2020 and nonâ€‘null Authors)\n",
    "clean_df = df.filter(~validation_expr)\n",
    "\n",
    "# Example aggregation: count publications per author\n",
    "agg_df = (\n",
    "    clean_df.groupBy(\"Authors\")\n",
    "            .agg(F.count(\"*\").alias(\"publication_count\"))\n",
    ")\n",
    "\n",
    "display(agg_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "etl_demo.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
